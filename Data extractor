import requests
from bs4 import BeautifulSoup
import time
import re

# Set the base URL for Flipkart search results
base_url = 'https://www.flipkart.com/search?q=Nike+casual+shoes'

# Create a session
session = requests.Session()

# Define headers, including a user-agent
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8'
}

# Function to scrape product URLs from the search results page
def scrape_product_urls(search_url, max_urls=100):
    urls = []  # List to hold product URLs
    product_link_classes = ['VJA3rP', 'rPDeLR', 'CGtC98','_4WELSP']  # List of possible class names

    while search_url and len(urls) < max_urls:  # Continue until max URLs are reached
        response = session.get(search_url, headers=headers)

        # Check if the request was successful
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Find all product links on the page using multiple classes
            product_links = []
            for class_name in product_link_classes:
                product_links.extend(soup.find_all('a', class_=class_name))

            # Extract and append URLs to the list
            for link in product_links:
                product_url = 'https://www.flipkart.com' + link['href']
                urls.append(product_url)
                if len(urls) >= max_urls:  # Stop if we reach the max limit
                    break

            # Find the next page link
            if len(urls) < max_urls:  # Only search for next page if we haven't reached the limit
                next_page = soup.find('a', class_='_1LKTO3')  # Adjust this class based on actual page structure
                if next_page:
                    search_url = 'https://www.flipkart.com' + next_page['href']
                    time.sleep(2)  # Sleep to avoid overwhelming the server
                else:
                    break  # Exit the loop if there is no next page
        else:
            print(f"Failed to retrieve the page. Status code: {response.status_code}")
            break

    return urls[:max_urls]  # Return only the top URLs

# Scrape top URLs for products
top_product_urls = scrape_product_urls(base_url)

# Function to scrape product data from a single URL
def scrape_product(url):
    # Send a GET request to the URL with headers
    response = session.get(url, headers=headers)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the HTML content
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find elements with updated or flexible class names
        price_tag = soup.find('div', class_='Nx9bqj CxhGGd')  # Class for price
        product_name_tag = soup.find('span', class_='VU-ZEz')  # Class for product name
        rating_tag = soup.find('div', class_='XQDdHH')  # Class for rating
        seller_tag = soup.find('div', class_='yeLeBC')  # Class for seller
        discount_tag = soup.find('div', class_='UkUFwK WW8yVX')  # Class for discount
        lowest_price_tag = soup.find('div', class_='hk-graph-min-price')  # Class for lowest price

        # If the price tag is found, extract and print the price
        if price_tag and product_name_tag:
            product_name = product_name_tag.text.strip()
            price = price_tag.text.strip()
            print(f"Product Name: {product_name}")
            print(f"Price: {price}")
            print(f"Link: Click here ({url})")

            # Extract rating and seller info if available
            rating = rating_tag.text.strip() if rating_tag else "N/A"
            seller = seller_tag.text.strip() if seller_tag else "N/A"
            discount = discount_tag.text.strip() if discount_tag else "N/A"
            lowest_price = lowest_price_tag.text.strip() if lowest_price_tag else "N/A"

            print(f"Rating: {rating}/5")
            print(f"Seller: {seller}")
            print(f"Discount: {discount}")
            print(f"Lowest Price: {lowest_price}")

            print('-' * 40)  # Separator for readability
        else:
            print(f"Product details not found for URL: {url}")
    else:
        print(f"Failed to retrieve the page. Status code: {response.status_code}")

# Scrape details from the collected product URLs
print("\n" + "="*50)
print("            Product Details            ")
print("="*50)

# Iterate through the list of URLs and scrape product details
for url in top_product_urls:
    scrape_product(url)
    time.sleep(5)  # Increase sleep duration between requests

